{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; font-size: 40px;\">\n",
    "    <b>Final Project</b>\n",
    "    <br>\n",
    "    Jarry Guillaume\n",
    "    <br>\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import arff\n",
    "import pandas as pd\n",
    "from kmedoids import KMedoids\n",
    "import os \n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project, we will use the categorical Datasets from the ADRepository-Anomaly-detection-datasets github repository. It is available here : \n",
    "\n",
    "- https://github.com/GuansongPang/ADRepository-Anomaly-detection-datasets?tab=readme-ov-fil\n",
    "\n",
    "Since our article is focused on aonmaly detection for discrete timeseries, these dataset will allow us to deploy some of the techniques showcased in the article. Let's start ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error while parsing file : ADRepository-Anomaly-detection-datasets/categorical data\\census-income-full-nominal.tar.xz\n",
      "Error while parsing file : ADRepository-Anomaly-detection-datasets/categorical data\\covertype_nominal_4vs123567.tar.xz\n",
      "Error while parsing file : ADRepository-Anomaly-detection-datasets/categorical data\\Reuters-corn-100.arff\n",
      "Error while parsing file : ADRepository-Anomaly-detection-datasets/categorical data\\w7a-libsvm-nonsparse.tar.xz\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"ADRepository-Anomaly-detection-datasets/categorical data/\"\n",
    "datasets = []\n",
    "\n",
    "for filepath in glob.glob(os.path.join(folder_path, \"*\")):\n",
    "    try: \n",
    "        data, meta = arff.loadarff(filepath)\n",
    "        datasets.append((data, meta))\n",
    "    except: \n",
    "        print(f\"Error while parsing file : {filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro : Generating Synthetic Data :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the data we tried to find online for synthetic timeseries was rarely labeled, we propose to generate some synthetic data so that we can test and implement as many algorithm for our discreet anomaly detection library. We can then test our algorithm on some real, less labeled data.\n",
    "\n",
    "### Markovian models :     \n",
    "\n",
    "This class will generate synthetic data that creates Markovian Discreet sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHABET = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkovianSequences: \n",
    "    def __init__(self, transition_matrix, hidden_matrix=None, n_sequences=100, sequence_length=50): \n",
    "        self.transition_matrix = transition_matrix\n",
    "        self.n_symbols = len(transition_matrix)\n",
    "        self.symbols = ALPHABET[:self.n_symbols]\n",
    "        self.sequence_length = sequence_length\n",
    "        self.n_sequences = n_sequences\n",
    "        self.hidden_matrix = hidden_matrix\n",
    "        self.check_probabilities()\n",
    "    \n",
    "    def check_probabilities(self): \n",
    "        for i in range(self.transition_matrix.shape[0]):\n",
    "            if not np.isclose(np.sum(self.transition_matrix[i]), 1.0):\n",
    "                raise ValueError(f\"Row {i} of transition_matrix does not sum to 1.\")\n",
    "\n",
    "        if self.hidden_matrix is not None:\n",
    "            if self.hidden_matrix.shape[0] != self.hidden_matrix.shape[1]:\n",
    "                raise ValueError(\"hidden_matrix must be square.\")\n",
    "            for i in range(self.hidden_matrix.shape[0]):\n",
    "                if not np.isclose(np.sum(self.hidden_matrix[i]), 1.0):\n",
    "                    raise ValueError(f\"Row {i} of hidden_matrix does not sum to 1.\")\n",
    "            self.n_hidden = self.hidden_matrix.shape[0]\n",
    "            if self.n_hidden != self.transition_matrix.shape[0]:\n",
    "                raise ValueError(\"Number of hidden states does not match the dimension of transition_matrix.\")\n",
    "            \n",
    "    def generate_sequence(self, initial_state=None): \n",
    "        if initial_state is None:\n",
    "            current_state = np.random.choice(self.n_symbols)\n",
    "        else:\n",
    "            current_state = initial_state\n",
    "\n",
    "        sequence = [self.symbols[current_state]]\n",
    "        for _ in range(self.sequence_length - 1):\n",
    "            next_state = np.random.choice(self.n_symbols, p=self.transition_matrix[current_state])\n",
    "            sequence.append(self.symbols[next_state])\n",
    "            current_state = next_state\n",
    "        return sequence\n",
    "    \n",
    "    def generate_hidden_sequence(self, initial_state=None):       \n",
    "        if initial_state is None:\n",
    "            current_hidden_state = np.random.choice(self.n_hidden)\n",
    "        else:\n",
    "            current_hidden_state = initial_state\n",
    "\n",
    "        current_symbol = np.random.choice(self.n_symbols, p=self.transition_matrix[current_hidden_state])\n",
    "        sequence = [self.symbols[current_symbol]]\n",
    "\n",
    "        for _ in range(self.sequence_length - 1):\n",
    "            next_hidden_state = np.random.choice(self.n_hidden, p=self.hidden_matrix[current_hidden_state])\n",
    "            emitted_symbol = np.random.choice(self.n_symbols, p=self.transition_matrix[next_hidden_state])\n",
    "            sequence.append(self.symbols[emitted_symbol])\n",
    "            current_hidden_state = next_hidden_state\n",
    "\n",
    "        return sequence\n",
    "\n",
    "    def generate_all_sequences(self):\n",
    "        all_seqs = []\n",
    "        for _ in range(self.n_sequences):\n",
    "            if self.hidden_matrix is not None: \n",
    "                seq = self.generate_hidden_sequence()\n",
    "            else: \n",
    "                seq = self.generate_sequence()\n",
    "            all_seqs.append(seq)\n",
    "        return all_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkovianDatasetGenerator: \n",
    "    def __init__(self, transition_matrices, hidden_matrices, n_sequences=100, sequence_length=50): \n",
    "        self.transition_matrices = transition_matrices\n",
    "        self.hidden_matrices = hidden_matrices \n",
    "        self.n_sequences = n_sequences\n",
    "        self.sequence_length = sequence_length\n",
    "        self.generators = self.init_transform()\n",
    "    \n",
    "    def init_transform(self):\n",
    "        self.generators = []\n",
    "        self.dataset = []\n",
    "        for transition_matrix, hidden_matrix in zip(self.transition_matrices, self.hidden_matrices): \n",
    "            generator = MarkovianSequences(transition_matrix, \n",
    "                                           hidden_matrix=hidden_matrix, \n",
    "                                           n_sequences=self.n_sequences, \n",
    "                                           sequence_length=self.sequence_length) \n",
    "            self.generators.append(generator)\n",
    "        return self.generators\n",
    "\n",
    "    def generate(self): \n",
    "        self.dataset = []\n",
    "        for generator in self.generators: \n",
    "            sequences = generator.generate_all_sequences()\n",
    "            self.dataset.extend(sequences)\n",
    "\n",
    "        return self.dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let us generate a dataset mixing a hidden Markow model and two markow models and let us wrap them up into the same dataset. We will also add an anomaly dataset, which will be another Markov model, with different probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_kernel_example = np.array([\n",
    "    [0.7, 0.2, 0.1],\n",
    "    [0.1, 0.7, 0.2],\n",
    "    [0.2, 0.1, 0.7]\n",
    "])\n",
    "\n",
    "hidden_matrix = np.array([\n",
    "    [0.7, 0.2, 0.1],\n",
    "    [0.1, 0.8, 0.1],\n",
    "    [0.2, 0.2, 0.6]\n",
    "])\n",
    "\n",
    "transition_matrix_example = np.array([\n",
    "    [0.6, 0.3, 0.1],  # Emission distribution from hidden state 0\n",
    "    [0.2, 0.5, 0.3],  # Emission distribution from hidden state 1\n",
    "    [0.1, 0.2, 0.7]   # Emission distribution from hidden state 2\n",
    "])\n",
    "\n",
    "transition_kernel_example = np.array([\n",
    "    [0.7, 0.2, 0.1],\n",
    "    [0.1, 0.7, 0.2],\n",
    "    [0.2, 0.1, 0.7]\n",
    "])\n",
    "\n",
    "transition_matrices = []\n",
    "hidden_matrices = [hidden_matrix, ]\n",
    "\n",
    "generator = MarkovianDatasetGenerator(transition_matrices, hidden_matrices)\n",
    "dataset = generator.generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I) Kernell-Based Techniques "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KernellBase: \n",
    "    def __init__(self, dataset, similarity_metric): \n",
    "        self.dataset = dataset\n",
    "        self.similarity_metric = similarity_metric\n",
    "        self.similarity_matrix = None\n",
    "\n",
    "    def compute_similarity_matrix(self, n_clusters): \n",
    "        n = len(self.dataset)\n",
    "        self.similarity_matrix = np.zeros((n, n))\n",
    "        for i in range(n):\n",
    "            for j in range(i, n):\n",
    "                sim = self.similarity_metric(self.dataset[i], self.dataset[j])\n",
    "                self.similarity_matrix[i,j] = sim\n",
    "                self.similarity_matrix[j,i] = sim  # Symmetric\n",
    "        \n",
    "        self.distance_matrix = 1 - self.similarity_matrix\n",
    "        kmedoids = KMedoids(n_clusters=n_clusters, random_state=42)\n",
    "        kmedoids.fit(self.distance_matrix)\n",
    "        self.medoids = self.dataset[kmedoids.cluster_centers_]\n",
    "        return self.similarity_matrix\n",
    "\n",
    "    def knearest_predict(self, test_sequence, k_nearest=5):\n",
    "        similarities = []\n",
    "        for sequence in self.dataset:  \n",
    "            similarities.append(self.similarity_metric(test_sequence, sequence))\n",
    "        \n",
    "        similarities.sort(reverse=True)\n",
    "        anomaly_score = 1 / similarities[k_nearest]\n",
    "        return anomaly_score\n",
    "\n",
    "    def clustering_predict(self, test_sequence, n_clusters=5):\n",
    "        if self.similarity_matrix is None: \n",
    "            self.compute_similarity_matrix(n_clusters)\n",
    "\n",
    "        max_similarity = 0\n",
    "        for medoid in self.medoids: \n",
    "            max_similarity = max(max_similarity, self.similarity_metric(test_sequence, medoid))\n",
    "        \n",
    "        return 1 / max_similarity       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us try our Kernell based methods with the longest common sequence kernell suggested in the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LCS_length(seq1, seq2):\n",
    "    len1, len2 = len(seq1), len(seq2)\n",
    "    dp = [[0]*(len2+1) for _ in range(len1+1)]\n",
    "    for i in range(1, len1+1):\n",
    "        for j in range(1, len2+1):\n",
    "            if seq1[i-1] == seq2[j-1]:\n",
    "                dp[i][j] = dp[i-1][j-1] + 1\n",
    "            else:\n",
    "                dp[i][j] = max(dp[i-1][j], dp[i][j-1])\n",
    "    return dp[len1][len2]\n",
    "\n",
    "def nLCS(seq1, seq2):\n",
    "    lcs = LCS_length(seq1, seq2)\n",
    "    return lcs / ( (len(seq1)*len(seq2))**0.5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernell_based = KernellBase(dataset, nLCS)\n",
    "kernell_based.compute_similarity_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II) Window Based Techniques :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowBased: \n",
    "    def __init__(self, window_length): \n",
    "        self.window_length = window_length"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "timeseries",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
