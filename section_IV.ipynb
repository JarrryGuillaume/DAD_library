{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; font-size: 40px;\">\n",
    "    <b>Final Project</b>\n",
    "    <br>\n",
    "    Jarry Guillaume\n",
    "    <br>\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import arff\n",
    "import pandas as pd\n",
    "import kmedoids \n",
    "import os \n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project, we will use the categorical Datasets from the ADRepository-Anomaly-detection-datasets github repository. It is available here : \n",
    "\n",
    "- https://github.com/GuansongPang/ADRepository-Anomaly-detection-datasets?tab=readme-ov-fil\n",
    "\n",
    "Since our article is focused on aonmaly detection for discrete timeseries, these dataset will allow us to deploy some of the techniques showcased in the article. Let's start ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"ADRepository-Anomaly-detection-datasets/categorical data/\"\n",
    "datasets = []\n",
    "\n",
    "for filepath in glob.glob(os.path.join(folder_path, \"*\")):\n",
    "    try: \n",
    "        data, meta = arff.loadarff(filepath)\n",
    "        datasets.append((data, meta))\n",
    "    except: \n",
    "        print(f\"Error while parsing file : {filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro : Generating Synthetic Data :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the data we tried to find online for synthetic timeseries was rarely labeled, we propose to generate some synthetic data so that we can test and implement as many algorithm for our discreet anomaly detection library. We can then test our algorithm on some real, less labeled data.\n",
    "\n",
    "### Markovian models :     \n",
    "\n",
    "This class will generate synthetic data that creates Markovian Discreet sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHABET = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkovianSequences: \n",
    "    def __init__(self, transition_matrix, hidden_matrix=None, n_sequences=100, sequence_length=50): \n",
    "        self.transition_matrix = transition_matrix\n",
    "        self.n_symbols = len(transition_matrix)\n",
    "        self.symbols = ALPHABET[:self.n_symbols]\n",
    "        self.sequence_length = sequence_length\n",
    "        self.n_sequences = n_sequences\n",
    "        self.hidden_matrix = hidden_matrix\n",
    "        self.check_probabilities()\n",
    "    \n",
    "    def check_probabilities(self): \n",
    "        for i in range(self.transition_matrix.shape[0]):\n",
    "            if not np.isclose(np.sum(self.transition_matrix[i]), 1.0):\n",
    "                raise ValueError(f\"Row {i} of transition_matrix does not sum to 1.\")\n",
    "\n",
    "        if self.hidden_matrix is not None:\n",
    "            if self.hidden_matrix.shape[0] != self.hidden_matrix.shape[1]:\n",
    "                raise ValueError(\"hidden_matrix must be square.\")\n",
    "            for i in range(self.hidden_matrix.shape[0]):\n",
    "                if not np.isclose(np.sum(self.hidden_matrix[i]), 1.0):\n",
    "                    raise ValueError(f\"Row {i} of hidden_matrix does not sum to 1.\")\n",
    "            self.n_hidden = self.hidden_matrix.shape[0]\n",
    "            if self.n_hidden != self.transition_matrix.shape[0]:\n",
    "                raise ValueError(\"Number of hidden states does not match the dimension of transition_matrix.\")\n",
    "            \n",
    "    def generate_sequence(self, initial_state=None): \n",
    "        if initial_state is None:\n",
    "            current_state = np.random.choice(self.n_symbols)\n",
    "        else:\n",
    "            current_state = initial_state\n",
    "\n",
    "        sequence = [self.symbols[current_state]]\n",
    "        for _ in range(self.sequence_length - 1):\n",
    "            next_state = np.random.choice(self.n_symbols, p=self.transition_matrix[current_state])\n",
    "            sequence.append(self.symbols[next_state])\n",
    "            current_state = next_state\n",
    "        return sequence\n",
    "    \n",
    "    def generate_hidden_sequence(self, initial_state=None):       \n",
    "        if initial_state is None:\n",
    "            current_hidden_state = np.random.choice(self.n_hidden)\n",
    "        else:\n",
    "            current_hidden_state = initial_state\n",
    "\n",
    "        current_symbol = np.random.choice(self.n_symbols, p=self.transition_matrix[current_hidden_state])\n",
    "        sequence = [self.symbols[current_symbol]]\n",
    "\n",
    "        for _ in range(self.sequence_length - 1):\n",
    "            next_hidden_state = np.random.choice(self.n_hidden, p=self.hidden_matrix[current_hidden_state])\n",
    "            emitted_symbol = np.random.choice(self.n_symbols, p=self.transition_matrix[next_hidden_state])\n",
    "            sequence.append(self.symbols[emitted_symbol])\n",
    "            current_hidden_state = next_hidden_state\n",
    "\n",
    "        return sequence\n",
    "\n",
    "    def generate_all_sequences(self):\n",
    "        all_seqs = []\n",
    "        for _ in range(self.n_sequences):\n",
    "            if self.hidden_matrix is not None: \n",
    "                seq = self.generate_hidden_sequence()\n",
    "            else: \n",
    "                seq = self.generate_sequence()\n",
    "            all_seqs.append(seq)\n",
    "        return all_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkovianDatasetGenerator: \n",
    "    def __init__(self, transition_matrices, hidden_matrices, n_sequences=100, sequence_length=50): \n",
    "        self.transition_matrices = transition_matrices\n",
    "        self.hidden_matrices = hidden_matrices \n",
    "        self.n_sequences = n_sequences\n",
    "        self.sequence_length = sequence_length\n",
    "        self.generators = self.init_transform()\n",
    "    \n",
    "    def init_transform(self):\n",
    "        self.generators = []\n",
    "        self.dataset = []\n",
    "        for transition_matrix, hidden_matrix in zip(self.transition_matrices, self.hidden_matrices): \n",
    "            generator = MarkovianSequences(transition_matrix, \n",
    "                                           hidden_matrix=hidden_matrix, \n",
    "                                           n_sequences=self.n_sequences, \n",
    "                                           sequence_length=self.sequence_length) \n",
    "            self.generators.append(generator)\n",
    "        return self.generators\n",
    "\n",
    "    def generate(self): \n",
    "        self.dataset = []\n",
    "        for generator in self.generators: \n",
    "            sequences = generator.generate_all_sequences()\n",
    "            self.dataset.extend(sequences)\n",
    "\n",
    "        return self.dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let us generate a dataset mixing a hidden Markow model and two markow models and let us wrap them up into the same dataset. We will also add an anomaly dataset, which will be another Markov model, with different probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_matrix1 = np.array([\n",
    "    [0.7, 0.2, 0.1],\n",
    "    [0.1, 0.7, 0.2],\n",
    "    [0.2, 0.1, 0.7]\n",
    "])\n",
    "\n",
    "hidden_matrix1 = np.array([\n",
    "    [0.7, 0.2, 0.1],\n",
    "    [0.1, 0.8, 0.1],\n",
    "    [0.2, 0.2, 0.6]\n",
    "])\n",
    "\n",
    "transition_matrix2 = np.array([\n",
    "    [0.6, 0.3, 0.1],  # Emission distribution from hidden state 0\n",
    "    [0.2, 0.5, 0.3],  # Emission distribution from hidden state 1\n",
    "    [0.1, 0.2, 0.7]   # Emission distribution from hidden state 2\n",
    "])\n",
    "\n",
    "N = 5  \n",
    "transition_matrix3 = np.random.rand(N, N)  \n",
    "row_sums = transition_matrix3.sum(axis=1, keepdims=True)\n",
    "transition_matrix3 = transition_matrix3 / row_sums\n",
    "\n",
    "transition_matrices = [transition_matrix1, transition_matrix2, transition_matrix3]\n",
    "hidden_matrices = [hidden_matrix1, None, None ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = MarkovianDatasetGenerator(transition_matrices, hidden_matrices, n_sequences=200, sequence_length=50)\n",
    "train_dataset = generator.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 3\n",
    "transition_matrix4 = np.random.rand(N, N)  \n",
    "row_sums = transition_matrix4.sum(axis=1, keepdims=True)\n",
    "transition_matrix4 = transition_matrix4 / row_sums\n",
    "test_generator = MarkovianDatasetGenerator([transition_matrix4, transition_matrix3], [None, None], n_sequences=20, sequence_length=50)\n",
    "test_dataset = test_generator.generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I) Kernell-Based Techniques "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KernellBase: \n",
    "    def __init__(self, dataset, similarity_metric): \n",
    "        self.dataset = dataset\n",
    "        self.similarity_metric = similarity_metric\n",
    "        self.similarity_matrix = None\n",
    "        self.medoids = None\n",
    "\n",
    "    def compute_similarity_matrix(self): \n",
    "        n = len(self.dataset)\n",
    "        self.similarity_matrix = np.zeros((n, n))\n",
    "        for i in range(n):\n",
    "            for j in range(i, n):\n",
    "                sim = self.similarity_metric(self.dataset[i], self.dataset[j])\n",
    "                self.similarity_matrix[i,j] = sim\n",
    "                self.similarity_matrix[j,i] = sim  # Symmetric\n",
    "        \n",
    "        self.distance_matrix = 1 - self.similarity_matrix\n",
    "        return self.similarity_matrix\n",
    "    \n",
    "    def compute_kemedoids(self, kmax=10, kmin=1): \n",
    "        km = kmedoids.dynmsc(self.distance_matrix, kmax, kmin)\n",
    "        self.medoids = [self.dataset[medoid] for medoid in km.medoids]\n",
    "        return self.medoids\n",
    "\n",
    "    def knearest_predict(self, test_sequence, k_nearest=5):\n",
    "        similarities = []\n",
    "        for sequence in self.dataset:  \n",
    "            similarities.append(self.similarity_metric(test_sequence, sequence))\n",
    "        \n",
    "        similarities.sort(reverse=True)\n",
    "        anomaly_score = 1 / similarities[k_nearest]\n",
    "        return anomaly_score\n",
    "\n",
    "    def clustering_predict(self, test_sequence, kmax=10, kmin=1):\n",
    "        if self.similarity_matrix is None: \n",
    "            self.compute_similarity_matrix()\n",
    "        if self.medoids is None:\n",
    "            self.compute_kemedoids(kmax=kmax, kmin=kmin)\n",
    "\n",
    "        max_similarity = 0\n",
    "        for medoid in self.medoids: \n",
    "            max_similarity = max(max_similarity, self.similarity_metric(test_sequence, medoid))\n",
    "        \n",
    "        return 1 / max_similarity       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us try our Kernell based methods with the longest common sequence kernell suggested in the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LCS_length(seq1, seq2):\n",
    "    len1, len2 = len(seq1), len(seq2)\n",
    "    dp = [[0]*(len2+1) for _ in range(len1+1)]\n",
    "    for i in range(1, len1+1):\n",
    "        for j in range(1, len2+1):\n",
    "            if seq1[i-1] == seq2[j-1]:\n",
    "                dp[i][j] = dp[i-1][j-1] + 1\n",
    "            else:\n",
    "                dp[i][j] = max(dp[i-1][j], dp[i][j-1])\n",
    "    return dp[len1][len2]\n",
    "\n",
    "def nLCS(seq1, seq2):\n",
    "    lcs = LCS_length(seq1, seq2)\n",
    "    return lcs / ( (len(seq1)*len(seq2))**0.5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernell_based = KernellBase(train_dataset, nLCS)\n",
    "distance_matrix = kernell_based.compute_similarity_matrix()\n",
    "medoids = kernell_based.compute_kemedoids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0833333333333335"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II) Window Based Techniques :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "class WindowUnsupervisedSVM: \n",
    "    def __init__(self, dataset, n_symbols):\n",
    "        self.n_symbols = n_symbols\n",
    "        self.dataset = self.one_hot_encoding(dataset)\n",
    "\n",
    "    def one_hot_encoding(self): \n",
    "        one_hot_encoded = []\n",
    "        for seq in self.sequences:\n",
    "            mat = np.zeros((self.n_symbols, len(seq)), dtype=int)\n",
    "\n",
    "            for pos, symbol in enumerate(seq):\n",
    "                s_idx = self.symbol_to_idx[symbol]\n",
    "                mat[s_idx, pos] = 1\n",
    "\n",
    "            one_hot_encoded.append(mat)\n",
    "\n",
    "        return one_hot_encoded\n",
    "    \n",
    "    def train_classifier(self): \n",
    "        self.svm = OneClassSVM(gamma='auto').fit(self.dataset)\n",
    "        return self.svm\n",
    "    \n",
    "    def predict(self, test): \n",
    "        test = self.one_hot_encoding(test)\n",
    "        return self.svm.predict(test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "class WindowBased: \n",
    "    def __init__(self, dataset, window_length): \n",
    "        self.window_length = window_length\n",
    "        self.dataset = self.partition(dataset)\n",
    "\n",
    "    def partition(self, dataset): \n",
    "        partition = []\n",
    "        for sequence in dataset: \n",
    "            partition = [sequence[i*self.window_length:(i+1)*self.window_length] for i in range(int(len(sequence) // self.window_length))]\n",
    "            partition.append(partition)\n",
    "        \n",
    "        return partition\n",
    "    \n",
    "    def train_lookahead_pair(self, k_look_ahead=5): \n",
    "        self.lookahead_dict = {}\n",
    "        self.k_look_ahead = k_look_ahead\n",
    "        for partition in self.dataset:\n",
    "            for seq in partition: \n",
    "                for i in range(len(seq) - k_look_ahead):\n",
    "                    pair = (seq[i], seq[i + k_look_ahead])\n",
    "                    self.lookahead_dict[pair] = self.lookahead_dict.get(pair, 0) + 1\n",
    "        return self.lookahead_dict\n",
    "\n",
    "    def test_lookahead_pairs(self, test_dataset): \n",
    "        test_dataset = self.partition(test_dataset)\n",
    "\n",
    "        anomaly_score = []\n",
    "        for partition in test_dataset: \n",
    "            for sequence in partition:\n",
    "                anomalies = []\n",
    "                for i in range(len(sequence) - self.k_look_ahead):\n",
    "                    pair = (sequence[i], sequence[i + self.k_look_ahead])\n",
    "                    anomalies.append(self.lookahead_dict.get(pair, 0))\n",
    "\n",
    "            anomaly_score.append()\n",
    "\n",
    "        return anomaly_score\n",
    "    \n",
    "    def train_normal_dictionary(self): \n",
    "        self.frequency_dictionary = {}\n",
    "        for partition in self.dataset: \n",
    "            for sequence in partition: \n",
    "                sequence = tuple(sequence)\n",
    "                if sequence in self.frequency_dictionary.keys():\n",
    "                    self.frequency_dictionary[tuple(sequence)] = self.frequency_dictionary.get(sequence) + 1\n",
    "\n",
    "        return self.frequency_dictionary\n",
    "\n",
    "    def test_normal_dictionary(self, test_dataset):\n",
    "        test_dataset = self.partition(test_dataset) \n",
    "\n",
    "        anomaly_scores = []\n",
    "        for partition in test_dataset: \n",
    "            anomalies = []\n",
    "            for sequence in partition: \n",
    "                anomalies.append(self.frequency_dictionary.get(tuple(sequence)))\n",
    "            \n",
    "            score = self.process_anomaly(anomalies)\n",
    "            anomaly_scores.append(score)\n",
    "\n",
    "        return anomaly_scores\n",
    "    \n",
    "    def t_side(self): \n",
    "        return anomaly_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "timeseries",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
