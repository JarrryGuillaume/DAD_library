{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; font-size: 40px;\">\n",
    "    <b>Final Project</b>\n",
    "    <br>\n",
    "    Jarry Guillaume\n",
    "    <br>\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import arff\n",
    "import pandas as pd\n",
    "import os \n",
    "import glob\n",
    "from sklearn.svm import OneClassSVM\n",
    "from hmmlearn.hmm import CategoricalHMM\n",
    "from sklearn.svm import SVC\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project, we will use the categorical Datasets from the ADRepository-Anomaly-detection-datasets github repository. It is available here : \n",
    "\n",
    "- https://github.com/GuansongPang/ADRepository-Anomaly-detection-datasets?tab=readme-ov-fil\n",
    "\n",
    "Since our article is focused on aonmaly detection for discrete timeseries, these dataset will allow us to deploy some of the techniques showcased in the article. Let's start ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"ADRepository-Anomaly-detection-datasets/categorical data/\"\n",
    "datasets = []\n",
    "\n",
    "for filepath in glob.glob(os.path.join(folder_path, \"*\")):\n",
    "    try: \n",
    "        data, meta = arff.loadarff(filepath)\n",
    "        datasets.append((data, meta))\n",
    "    except: \n",
    "        print(f\"Error while parsing file : {filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro : Generating Synthetic Data :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the data we tried to find online for synthetic timeseries was rarely labeled, we propose to generate some synthetic data so that we can test and implement as many algorithm for our discreet anomaly detection library. We can then test our algorithm on some real, less labeled data.\n",
    "\n",
    "### Markovian models :     \n",
    "\n",
    "This class will generate synthetic data that creates Markovian Discreet sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHABET = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let us generate a dataset mixing a hidden Markow model and two markow models and let us wrap them up into the same dataset. We will also add an anomaly dataset, which will be another Markov model, with different probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_matrix1 = np.array([\n",
    "    [0.7, 0.2, 0.1],\n",
    "    [0.1, 0.7, 0.2],\n",
    "    [0.2, 0.1, 0.7]\n",
    "])\n",
    "\n",
    "hidden_matrix1 = np.array([\n",
    "    [0.7, 0.2, 0.1],\n",
    "    [0.1, 0.8, 0.1],\n",
    "    [0.2, 0.2, 0.6]\n",
    "])\n",
    "\n",
    "transition_matrix2 = np.array([\n",
    "    [0.6, 0.3, 0.1],  # Emission distribution from hidden state 0\n",
    "    [0.2, 0.5, 0.3],  # Emission distribution from hidden state 1\n",
    "    [0.1, 0.2, 0.7]   # Emission distribution from hidden state 2\n",
    "])\n",
    "\n",
    "N = 5  \n",
    "transition_matrix3 = np.random.rand(N, N)  \n",
    "row_sums = transition_matrix3.sum(axis=1, keepdims=True)\n",
    "transition_matrix3 = transition_matrix3 / row_sums\n",
    "\n",
    "transition_matrices = [transition_matrix1, transition_matrix2, transition_matrix3]\n",
    "hidden_matrices = [hidden_matrix1, None, None ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable MarkovianDatasetGenerator object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[338], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m generator, labels \u001b[38;5;241m=\u001b[39m MarkovianDatasetGenerator(transition_matrices, hidden_matrices, n_sequences\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m, sequence_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[0;32m      2\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m generator\u001b[38;5;241m.\u001b[39mgenerate()\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable MarkovianDatasetGenerator object"
     ]
    }
   ],
   "source": [
    "generator, labels = MarkovianDatasetGenerator(transition_matrices, hidden_matrices, n_sequences=200, sequence_length=50)\n",
    "train_dataset = generator.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 3\n",
    "transition_matrix4 = np.random.rand(N, N)  \n",
    "row_sums = transition_matrix4.sum(axis=1, keepdims=True)\n",
    "transition_matrix4 = transition_matrix4 / row_sums\n",
    "test_generator = MarkovianDatasetGenerator([transition_matrix4, transition_matrix3], [None, None], n_sequences=20, sequence_length=50)\n",
    "test_dataset = test_generator.generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I) Kernell-Based Techniques "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us try our Kernell based methods with the longest common sequence kernell suggested in the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernell_based = KernellBase(train_dataset, nLCS)\n",
    "distance_matrix = kernell_based.compute_similarity_matrix()\n",
    "medoids = kernell_based.compute_kemedoids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II) Window Based Techniques :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Window Based Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lookahead Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Dictionary method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T-side Algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TSIDE(WindowBasedStruct): \n",
    "    def __init__(self, dataset, window_length, mode=\"average\"): \n",
    "        super().__init__(dataset, window_length, mode=mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV) Hidden Markov Models :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "class ViterbiTrainerHMM:\n",
    "    def __init__(self, n_states, n_symbols, max_iter=50, tol=1e-3):\n",
    "        self.n_states = n_states\n",
    "        self.n_symbols = n_symbols\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "\n",
    "        A = np.random.rand(n_states, n_states)\n",
    "        self.A = A / A.sum(axis=1, keepdims=True)\n",
    "\n",
    "        B = np.random.rand(n_states, n_symbols)\n",
    "        self.B = B / B.sum(axis=1, keepdims=True)\n",
    "\n",
    "        self.pi = np.full(n_states, 1.0/n_states)\n",
    "\n",
    "    def _viterbi(self, O):\n",
    "        \"\"\"\n",
    "        Viterbi algorithm for a single sequence O:\n",
    "        O: array of shape (T,) containing integer-coded observations\n",
    "        returns:\n",
    "          states: most likely state sequence\n",
    "          prob: probability of that sequence under the model\n",
    "        \"\"\"\n",
    "        T = len(O)\n",
    "        delta = np.zeros((T, self.n_states))\n",
    "        psi = np.zeros((T, self.n_states), dtype=int)\n",
    "\n",
    "        # Initialization\n",
    "        delta[0, :] = self.pi * self.B[:, O[0]]\n",
    "        psi[0, :] = 0\n",
    "\n",
    "        # Recursion\n",
    "        for t in range(1, T):\n",
    "            for j in range(self.n_states):\n",
    "                vals = delta[t-1, :] * self.A[:, j]\n",
    "                i_star = np.argmax(vals)\n",
    "                delta[t, j] = vals[i_star] * self.B[j, O[t]]\n",
    "                psi[t, j] = i_star\n",
    "\n",
    "        # Termination\n",
    "        p_star = np.max(delta[T-1, :])\n",
    "        q_star = np.zeros(T, dtype=int)\n",
    "        q_star[T-1] = np.argmax(delta[T-1, :])\n",
    "\n",
    "        # Backtrack\n",
    "        for t in range(T-2, -1, -1):\n",
    "            q_star[t] = psi[t+1, q_star[t+1]]\n",
    "\n",
    "        return q_star, p_star\n",
    "\n",
    "    def fit(self, dataset):\n",
    "        \"\"\"\n",
    "        Fit the HMM parameters to the dataset using Viterbi training.\n",
    "        dataset: list of sequences, each sequence is a list or array of integer-coded observations.\n",
    "        \"\"\"\n",
    "        prev_log_likelihood = -np.inf\n",
    "        for iteration in range(self.max_iter):\n",
    "            # Accumulators for counts\n",
    "            pi_counts = np.zeros(self.n_states)\n",
    "            A_counts = np.zeros((self.n_states, self.n_states))\n",
    "            B_counts = np.zeros((self.n_states, self.n_symbols))\n",
    "\n",
    "            total_log_likelihood = 0.0\n",
    "\n",
    "            # For each sequence, run Viterbi to get most likely path\n",
    "            for O in dataset:\n",
    "                states, seq_prob = self._viterbi(O)\n",
    "                total_log_likelihood += np.log(seq_prob + 1e-12)\n",
    "\n",
    "                # Update counts based on the Viterbi path\n",
    "                pi_counts[states[0]] += 1\n",
    "                for t in range(len(O)):\n",
    "                    B_counts[states[t], O[t]] += 1\n",
    "                    if t > 0:\n",
    "                        A_counts[states[t-1], states[t]] += 1\n",
    "\n",
    "            # Normalize to get new parameters\n",
    "            self.pi = pi_counts / pi_counts.sum() if pi_counts.sum() > 0 else self.pi\n",
    "\n",
    "            row_sums_A = A_counts.sum(axis=1, keepdims=True)\n",
    "            row_sums_A[row_sums_A == 0] = 1e-12\n",
    "            self.A = A_counts / row_sums_A\n",
    "\n",
    "            row_sums_B = B_counts.sum(axis=1, keepdims=True)\n",
    "            row_sums_B[row_sums_B == 0] = 1e-12\n",
    "            self.B = B_counts / row_sums_B\n",
    "\n",
    "            # Check for convergence\n",
    "            if np.abs(total_log_likelihood - prev_log_likelihood) < self.tol:\n",
    "                break\n",
    "            prev_log_likelihood = total_log_likelihood\n",
    "\n",
    "    def predict(self, dataset):\n",
    "        \"\"\"\n",
    "        Predict the most likely state sequences for a batch of sequences.\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        for O in dataset:\n",
    "            states, _ = self._viterbi(O)\n",
    "            predictions.append(states)\n",
    "        return predictions\n",
    "\n",
    "    def score(self, dataset):\n",
    "        \"\"\"\n",
    "        Compute the total log-likelihood of the dataset under the current model parameters using Viterbi best paths.\n",
    "        This is not the full likelihood (like forward algorithm gives), but the likelihood of the Viterbi path.\n",
    "        \"\"\"\n",
    "        total_log_likelihood = 0.0\n",
    "        for O in dataset:\n",
    "            _, seq_prob = self._viterbi(O)\n",
    "            total_log_likelihood += np.log(seq_prob + 1e-12)\n",
    "        return total_log_likelihood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaumWelchHMM: \n",
    "    def __init__(self, n_symbols, n_states, max_iter=100, tolerance=1e-3):\n",
    "        self.n_symbols = n_symbols\n",
    "        self.n_states = n_states\n",
    "        self.symbols = {elt:i for i, elt in enumerate(ALPHABET[:n_symbols])}\n",
    "\n",
    "        self.tolerance = tolerance \n",
    "        self.max_iter = max_iter\n",
    "\n",
    "        A = np.random.rand(n_states, n_states)\n",
    "        self.transition_matrix = A / A.sum(axis=1, keepdims=True)\n",
    "\n",
    "        B = np.random.rand(n_states, n_symbols)\n",
    "        self.emission_matrix  = B / B.sum(axis=1, keepdims=True)\n",
    "\n",
    "        self.initial_state = np.full(self.n_states, 1/self.n_states)\n",
    "\n",
    "    def init_fraction(self): \n",
    "        self.gamma_first = np.zeros(self.n_states)\n",
    "        self.transition_numerator = np.zeros((self.n_states, self.n_states))\n",
    "        self.emission_numerator = np.zeros((self.n_states, self.n_symbols))    \n",
    "        self.transition_denominator = np.zeros((self.n_states))  \n",
    "        self.emission_denominator = np.zeros((self.n_states))  \n",
    "\n",
    "    def one_hot_encode(self, sequence):\n",
    "        one_hot_encoded = np.zeros((len(sequence), self.n_symbols), dtype=int)\n",
    "        one_hot_encoded[np.arange(len(sequence)), sequence] = 1\n",
    "        return one_hot_encoded\n",
    "\n",
    "    def forward_scaled(self, sequence):\n",
    "        T = len(sequence)\n",
    "        alpha = np.zeros((T, self.n_states))\n",
    "        c = np.zeros(T)  # scaling factors\n",
    "\n",
    "        # Initialization\n",
    "        for i in range(self.n_states):\n",
    "            alpha[0, i] = self.initial_state[i] * self.emission_matrix[i, sequence[0]]\n",
    "        c[0] = 1.0 / (np.sum(alpha[0, :]) + 1e-300)\n",
    "        alpha[0, :] *= c[0]\n",
    "\n",
    "        # Recursion\n",
    "        for t in range(1, T):\n",
    "            for j in range(self.n_states):\n",
    "                alpha[t, j] = np.sum(alpha[t-1, :] * self.transition_matrix[:, j]) * self.emission_matrix[j, sequence[t]]\n",
    "            c[t] = 1.0 / (np.sum(alpha[t, :]) + 1e-300)\n",
    "            alpha[t, :] *= c[t]\n",
    "\n",
    "        return alpha, c\n",
    "\n",
    "    def backward_scaled(self, sequence, c):\n",
    "        T = len(sequence)\n",
    "        beta = np.zeros((T, self.n_states))\n",
    "        beta[T-1, :] = 1.0 * c[T-1]  \n",
    "\n",
    "        for t in range(T-2, -1, -1):\n",
    "            for i in range(self.n_states):\n",
    "                beta[t, i] = np.sum(self.transition_matrix[i, :] * self.emission_matrix[:, sequence[t+1]] * beta[t+1, :])\n",
    "            beta[t, :] *= c[t]\n",
    "\n",
    "        return beta\n",
    "\n",
    "    \n",
    "    def convert_dataset(self, dataset): \n",
    "        new_dataset = []\n",
    "        for sequence in dataset: \n",
    "            number_sequence = [self.symbols[letter] for letter in sequence]\n",
    "            new_dataset.append(number_sequence)\n",
    "        return new_dataset\n",
    "    \n",
    "    def E_step(self, sequence):\n",
    "        n = len(sequence)\n",
    "        alpha, c = self.forward_scaled(sequence)\n",
    "        beta = self.backward_scaled(sequence, c)\n",
    "\n",
    "        log_prob = -np.sum(np.log(c + 1e-300))\n",
    "        probability = np.exp(log_prob)\n",
    "\n",
    "        gamma = (alpha * beta)  \n",
    "        gamma = gamma / (np.sum(gamma, axis=1, keepdims=True) + 1e-300)\n",
    "\n",
    "        xi = np.zeros((n-1, self.n_states, self.n_states))\n",
    "        for t in range(n-1):\n",
    "            denom = np.sum(alpha[t, :] * beta[t, :]) + 1e-300\n",
    "            for i in range(self.n_states):\n",
    "                xi[t, i, :] = (alpha[t, i] * self.transition_matrix[i, :] *\n",
    "                            self.emission_matrix[:, sequence[t+1]] * beta[t+1, :]) / denom\n",
    "\n",
    "        return gamma, xi, probability\n",
    "\n",
    "\n",
    "    def M_step(self): \n",
    "        # print(self.gamma_first)\n",
    "        # print(self.transition_numerator, self.transition_denominator)\n",
    "        # print(self.emission_numerator, self.emission_denominator)\n",
    "        # print(\"=====================================\")\n",
    "        self.initial_state = self.gamma_first\n",
    "        self.transition_matrix = self.transition_numerator / (self.transition_denominator[:, np.newaxis] + 1e-12)\n",
    "        self.emission_matrix = self.emission_numerator / (self.emission_denominator[:, np.newaxis] + 1e-12)\n",
    "\n",
    "        self.transition_matrix /= self.transition_matrix.sum(axis=1)\n",
    "        self.emission_matrix /= self.emission_matrix.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    \n",
    "    def Baum_Welch(self, dataset):\n",
    "        n_sequence = len(dataset)\n",
    "        dataset = self.convert_dataset(dataset)\n",
    "        self.init_fraction()  \n",
    "        \n",
    "        log_likelihood = 0.0\n",
    "        \n",
    "        for sequence in dataset:\n",
    "            mask = self.one_hot_encode(sequence)\n",
    "\n",
    "            gamma, xi, probability = self.E_step(sequence)\n",
    "\n",
    "            self.gamma_first += gamma[0, :] / n_sequence\n",
    "\n",
    "            self.transition_numerator += np.sum(xi[:-1], axis=0)\n",
    "            self.emission_numerator += gamma.T @ mask\n",
    "            self.transition_denominator += np.sum(gamma[:-1], axis=0) \n",
    "            self.emission_denominator += np.sum(gamma, axis=0)\n",
    "\n",
    "            log_likelihood += np.log(probability + 1e-12)\n",
    "        \n",
    "        self.M_step()\n",
    "\n",
    "        return log_likelihood\n",
    "\n",
    "    def fit(self, dataset):\n",
    "        prev_log_likelihood = -np.inf\n",
    "        log_likelihood = 0 \n",
    "\n",
    "        with tqdm(total=self.max_iter, desc=\"EM Algorithm Progress\", unit=\"step\") as pbar:\n",
    "            for i in range(self.max_iter):\n",
    "                prev_log_likelihood = log_likelihood  \n",
    "                log_likelihood = self.Baum_Welch(dataset)\n",
    "                pbar.set_postfix({\"Log-Likelihood\": log_likelihood})\n",
    "                pbar.update(1)\n",
    "                \n",
    "\n",
    "    def predict_sample(self, sequence): \n",
    "        alpha = self.forward(sequence)\n",
    "        probability = np.sum(alpha[-1, :])\n",
    "        return -np.log(probability + 1e-12)\n",
    "    \n",
    "    def predict(self, dataset): \n",
    "        dataset = self.convert_dataset(dataset)\n",
    "        scores = []\n",
    "        for sequence in dataset:\n",
    "            scores.append(self.predict_sample(sequence))\n",
    "        return np.array(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "class ImprovedHiddenMarkovModel:\n",
    "    def __init__(self, n_symbols, n_states, max_iter=100, tolerance=1e-4, \n",
    "                 smoothing=1e-10, random_seed=42):\n",
    "        \"\"\"\n",
    "        Improved HMM implementation with numerical stability\n",
    "        \n",
    "        Parameters:\n",
    "        - n_symbols: Number of possible observation symbols\n",
    "        - n_states: Number of hidden states\n",
    "        - max_iter: Maximum number of EM iterations\n",
    "        - tolerance: Convergence threshold for log-likelihood\n",
    "        - smoothing: Small value to prevent zero probabilities\n",
    "        - random_seed: Seed for reproducibility\n",
    "        \"\"\"\n",
    "        np.random.seed(random_seed)\n",
    "        self.n_symbols = n_symbols\n",
    "        self.n_states = n_states\n",
    "        self.max_iter = max_iter\n",
    "        self.tolerance = tolerance\n",
    "        self.smoothing = smoothing\n",
    "        \n",
    "        # Initialize parameters with small smoothing\n",
    "        self.transition_matrix = self._initialize_transition_matrix()\n",
    "        self.emission_matrix = self._initialize_emission_matrix()\n",
    "        self.initial_state = self._initialize_initial_state()\n",
    "\n",
    "    def _initialize_transition_matrix(self):\n",
    "        \"\"\"Initialize transition matrix with smoothing\"\"\"\n",
    "        A = np.random.random((self.n_states, self.n_states)) + self.smoothing\n",
    "        return A / A.sum(axis=1, keepdims=True)\n",
    "\n",
    "    def _initialize_emission_matrix(self):\n",
    "        \"\"\"Initialize emission matrix with smoothing\"\"\"\n",
    "        B = np.random.random((self.n_states, self.n_symbols)) + self.smoothing\n",
    "        return B / B.sum(axis=1, keepdims=True)\n",
    "\n",
    "    def _initialize_initial_state(self):\n",
    "        \"\"\"Initialize initial state probabilities\"\"\"\n",
    "        pi = np.random.random(self.n_states) + self.smoothing\n",
    "        return pi / pi.sum()\n",
    "\n",
    "    def _log_forward_algorithm(self, sequence):\n",
    "        \"\"\"\n",
    "        Perform forward algorithm in log space to prevent underflow\n",
    "        \n",
    "        Returns:\n",
    "        - log_alpha: Log-space forward probabilities\n",
    "        - log_prob: Log probability of the sequence\n",
    "        \"\"\"\n",
    "        log_initial = np.log(self.initial_state + self.smoothing)\n",
    "        log_transition = np.log(self.transition_matrix + self.smoothing)\n",
    "        log_emission = np.log(self.emission_matrix + self.smoothing)\n",
    "\n",
    "        log_alpha = np.zeros((len(sequence), self.n_states))\n",
    "        log_alpha[0] = log_initial + log_emission[:, sequence[0]]\n",
    "        \n",
    "        for t in range(1, len(sequence)):\n",
    "            for j in range(self.n_states):\n",
    "                log_alpha[t, j] = logsumexp(\n",
    "                    log_alpha[t-1] + log_transition[:, j]\n",
    "                ) + log_emission[j, sequence[t]]\n",
    "        \n",
    "        log_prob = logsumexp(log_alpha[-1])\n",
    "        return log_alpha, log_prob\n",
    "\n",
    "    def _log_backward_algorithm(self, sequence, log_prob):\n",
    "        \"\"\"\n",
    "        Perform backward algorithm in log space\n",
    "        \n",
    "        Returns:\n",
    "        Log-space backward probabilities\n",
    "        \"\"\"\n",
    "        log_transition = np.log(self.transition_matrix + self.smoothing)\n",
    "        log_emission = np.log(self.emission_matrix + self.smoothing)\n",
    "\n",
    "        log_beta = np.zeros((len(sequence), self.n_states))\n",
    "        log_beta[-1] = 0.0\n",
    "\n",
    "        for t in range(len(sequence)-2, -1, -1):\n",
    "            for i in range(self.n_states):\n",
    "                log_beta[t, i] = logsumexp(\n",
    "                    log_transition[i, :] + \n",
    "                    log_emission[:, sequence[t+1]] + \n",
    "                    log_beta[t+1]\n",
    "                )\n",
    "        \n",
    "        return log_beta\n",
    "\n",
    "    def _compute_gamma_xi(self, sequence, log_alpha, log_beta, log_prob):\n",
    "        \"\"\"\n",
    "        Compute posterior probabilities gamma and xi\n",
    "        \"\"\"\n",
    "        log_transition = np.log(self.transition_matrix + self.smoothing)\n",
    "        log_emission = np.log(self.emission_matrix + self.smoothing)\n",
    "\n",
    "        # Compute log gamma (state posteriors)\n",
    "        log_gamma = log_alpha + log_beta - log_prob\n",
    "        gamma = np.exp(log_gamma)\n",
    "\n",
    "        # Compute log xi (transition posteriors)\n",
    "        xi = np.zeros((len(sequence)-1, self.n_states, self.n_states))\n",
    "        for t in range(len(sequence)-1):\n",
    "            for i in range(self.n_states):\n",
    "                for j in range(self.n_states):\n",
    "                    xi[t, i, j] = np.exp(\n",
    "                        log_alpha[t, i] + \n",
    "                        log_transition[i, j] + \n",
    "                        log_emission[j, sequence[t+1]] + \n",
    "                        log_beta[t+1, j] - \n",
    "                        log_prob\n",
    "                    )\n",
    "            xi[t] /= xi[t].sum()\n",
    "\n",
    "        return gamma, xi\n",
    "\n",
    "    def fit(self, sequences):\n",
    "        \"\"\"\n",
    "        EM algorithm for parameter estimation\n",
    "        \n",
    "        Args:\n",
    "        sequences: List of sequences, where each sequence is a list of symbol indices\n",
    "        \"\"\"\n",
    "        prev_log_likelihood = -np.inf\n",
    "\n",
    "        for iteration in tqdm(range(self.max_iter), desc=\"EM Iterations\"):\n",
    "            # Accumulators for expected counts\n",
    "            total_gamma_0 = np.zeros(self.n_states)\n",
    "            total_transition = np.zeros_like(self.transition_matrix)\n",
    "            total_emission = np.zeros_like(self.emission_matrix)\n",
    "            log_likelihood = 0.0\n",
    "\n",
    "            # E-step\n",
    "            for sequence in sequences:\n",
    "                # Compute log-space forward and backward probabilities\n",
    "                log_alpha, log_prob = self._log_forward_algorithm(sequence)\n",
    "                log_beta = self._log_backward_algorithm(sequence, log_prob)\n",
    "                \n",
    "                # Compute posterior probabilities\n",
    "                gamma, xi = self._compute_gamma_xi(sequence, log_alpha, log_beta, log_prob)\n",
    "                \n",
    "                # Accumulate statistics\n",
    "                total_gamma_0 += gamma[0]\n",
    "                total_transition += xi.sum(axis=0)\n",
    "                \n",
    "                for t, sym in enumerate(sequence):\n",
    "                    total_emission[:, sym] += gamma[t]\n",
    "                \n",
    "                log_likelihood += log_prob\n",
    "\n",
    "            # M-step: Update parameters with smoothing\n",
    "            self.initial_state = total_gamma_0 / total_gamma_0.sum()\n",
    "            self.transition_matrix = (total_transition + self.smoothing) \n",
    "            self.transition_matrix /= self.transition_matrix.sum(axis=1, keepdims=True)\n",
    "            \n",
    "            self.emission_matrix = (total_emission + self.smoothing)\n",
    "            self.emission_matrix /= self.emission_matrix.sum(axis=1, keepdims=True)\n",
    "\n",
    "            # Check convergence\n",
    "            if np.abs(log_likelihood - prev_log_likelihood) < self.tolerance:\n",
    "                break\n",
    "            \n",
    "            prev_log_likelihood = log_likelihood\n",
    "\n",
    "        return self\n",
    "\n",
    "def logsumexp(x):\n",
    "    \"\"\"\n",
    "    Numerically stable log-sum-exp trick\n",
    "    \"\"\"\n",
    "    max_x = np.max(x)\n",
    "    return max_x + np.log(np.sum(np.exp(x - max_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_probability_matrix(n_rows, n_cols):\n",
    "    matrix = np.random.rand(n_rows, n_cols)  \n",
    "    row_sums = matrix.sum(axis=1, keepdims=True)\n",
    "    matrix = matrix / row_sums\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_symbols = 4\n",
    "n_states = 3\n",
    "\n",
    "transition_matrix1 = np.array([\n",
    "    [0.7, 0.2, 0.1],\n",
    "    [0.1, 0.7, 0.2],\n",
    "    [0.2, 0.1, 0.7], \n",
    "])\n",
    "\n",
    "emission_matrix1 = np.array([\n",
    "    [0.5, 0.2, 0.1, 0.2],\n",
    "    [0.1, 0.6, 0.1, 0.2],\n",
    "    [0.2, 0.2, 0.4, 0.2]\n",
    "])\n",
    "\n",
    "\n",
    "transition_matrix2 = generate_random_probability_matrix(n_states, n_states)\n",
    "emission_matrix2 = generate_random_probability_matrix(n_states, n_symbols)\n",
    "\n",
    "generator = MarkovianSequences(transition_matrix=transition_matrix1, \n",
    "                               emission_matrix=emission_matrix1, \n",
    "                               sequence_length=10, \n",
    "                               n_sequences=200)\n",
    "train_dataset = generator.generate_all_sequences(initial_state=0)\n",
    "\n",
    "\n",
    "dataset_generator = MarkovianDatasetGenerator(transition_matrices=[transition_matrix1, transition_matrix2], \n",
    "                                                 emission_matrices=[emission_matrix1, emission_matrix2], \n",
    "                                                 sequence_length=10,)\n",
    "\n",
    "test_dataset, labels = dataset_generator.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ViterbiTrainerHMM.__init__() got multiple values for argument 'n_states'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[409], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m claude_dataset \u001b[38;5;241m=\u001b[39m BaumWelchHMM(n_symbols, n_states\u001b[38;5;241m=\u001b[39mn_states, max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, tolerance\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m)\u001b[38;5;241m.\u001b[39mconvert_dataset(train_dataset)\n\u001b[1;32m----> 3\u001b[0m hmm \u001b[38;5;241m=\u001b[39m \u001b[43mViterbiTrainerHMM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_symbols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m hmm\u001b[38;5;241m.\u001b[39mfit(train_dataset[:\u001b[38;5;241m100\u001b[39m])\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# train_dataset = hmm.convert_dataset(train_dataset)\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# cat = CategoricalHMM(n_components=3, algorithm='viterbi', n_iter=10)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# cat.fit(train_dataset)\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: ViterbiTrainerHMM.__init__() got multiple values for argument 'n_states'"
     ]
    }
   ],
   "source": [
    "claude_dataset = BaumWelchHMM(n_symbols, n_states=n_states, max_iter=10, tolerance=1e-3).convert_dataset(train_dataset)\n",
    "\n",
    "hmm = ViterbiTrainerHMM(n_symbols, n_states=n_states, max_iter=100, tolerance=1e-3)\n",
    "hmm.fit(train_dataset[:100])\n",
    "\n",
    "# train_dataset = hmm.convert_dataset(train_dataset)\n",
    "# cat = CategoricalHMM(n_components=3, algorithm='viterbi', n_iter=10)\n",
    "# cat.fit(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_dataset = hmm.convert_dataset(test_dataset)\n",
    "hmm.emission_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 0, ..., 2, 2, 2])"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = cat.predict(test_dataset)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.73355959e-04, 5.74615691e-10, 9.99426643e-01],\n",
       "       [8.77011591e-01, 1.12507443e-01, 1.04809666e-02],\n",
       "       [7.73528788e-01, 2.09231999e-01, 1.72392127e-02],\n",
       "       ...,\n",
       "       [6.56708989e-01, 7.20222327e-02, 2.71268779e-01],\n",
       "       [6.56258090e-01, 1.87903224e-01, 1.55838686e-01],\n",
       "       [5.18080705e-01, 2.36853056e-01, 2.45066239e-01]])"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat.predict_proba(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[312], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mhmm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[297], line 125\u001b[0m, in \u001b[0;36mHiddenMarkovModel.predict\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset): \n\u001b[1;32m--> 125\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m     scores \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sequence \u001b[38;5;129;01min\u001b[39;00m dataset:\n",
      "Cell \u001b[1;32mIn[297], line 57\u001b[0m, in \u001b[0;36mHiddenMarkovModel.convert_dataset\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m     55\u001b[0m new_dataset \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sequence \u001b[38;5;129;01min\u001b[39;00m dataset: \n\u001b[1;32m---> 57\u001b[0m     number_sequence \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msymbols\u001b[49m\u001b[43m[\u001b[49m\u001b[43mletter\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m letter \u001b[38;5;129;01min\u001b[39;00m sequence]\n\u001b[0;32m     58\u001b[0m     new_dataset\u001b[38;5;241m.\u001b[39mappend(number_sequence)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_dataset\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "predictions = hmm.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[nan, nan, nan],\n",
       "       [nan, nan, nan],\n",
       "       [nan, nan, nan]])"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmm.transition_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1817328 , 0.42281351, 0.21939922, 0.17605447],\n",
       "       [0.34848902, 0.43344485, 0.15738849, 0.06067764],\n",
       "       [0.20968133, 0.33918133, 0.11578368, 0.33535367]])"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmm.emission_matrix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "timeseries",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
